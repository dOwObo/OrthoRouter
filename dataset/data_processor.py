#  dataset/data_processor.py
import os
import json
import pandas as pd
from datasets import Dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
import torch
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ä¹‹å¾Œåˆªé™¤ï¼Œæ¸¬è©¦ç”¨
from torch.utils.data import DataLoader, Subset
import random


class DataProcessor:
    def __init__(self, 
                 data_file: str, 
                 labels_file: str, 
                 peft_model_path: str, 
                 max_input_length: int = 512, 
                 max_label_length: int = 32,
                 config_dir: str = "configs"):
        """
        åˆå§‹åŒ–è³‡æ–™è™•ç†æµç¨‹ã€‚
        """
        self.data_file = data_file
        self.labels_file = labels_file
        self.peft_model_path = peft_model_path
        self.max_input_length = max_input_length
        self.max_label_length = max_label_length
        self.config_dir = config_dir

        # è®€å–è©²è³‡æ–™é›†çš„æç¤ºçš„æ–‡ä»¶
        self.task_config = self.load_json(os.path.join(self.config_dir, "task.json"))
        self.instruction_config = self.load_json(os.path.join(self.config_dir, "instruction_config.json"))

        # è®€å–è³‡æ–™
        self.data_df = pd.read_json(self.data_file)

        # è®€å–æ¨™ç±¤ä¸¦å»ºç«‹æ˜ å°„
        with open(self.labels_file, "r", encoding="utf-8") as f:
            self.labels_list = json.load(f)
        # å°‡æ¨™ç±¤è½‰æ›ç‚º Snake_Case
        self.labels_list = [label.replace(" ", "_") for label in self.labels_list]
        self.label_to_id = {label: idx for idx, label in enumerate(self.labels_list)}
        self.id_to_label = {idx: label for idx, label in enumerate(self.labels_list)}

        # å–å¾— `dataset/` æ‰€åœ¨çš„æ ¹ç›®éŒ„ï¼ˆ`OrthMoE/`ï¼‰
        ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # è¨­å®š `initial_model/t5-large/` çš„å®Œæ•´è·¯å¾‘
        base_model_path = os.path.join(ROOT_DIR, "initial_model/t5-large")

        # **ç¢ºä¿ `config.json` å­˜åœ¨ï¼Œå¦å‰‡æ”¹ç”¨ `initial_model/t5-large/`**
        if os.path.exists(os.path.join(self.peft_model_path, "config.json")):
            config_path = self.peft_model_path
        else:
            logger.warning(f"âš ï¸ è­¦å‘Š: `{self.peft_model_path}` ç¼ºå°‘ `config.json`ï¼Œæ”¹ç”¨ `{base_model_path}`")
            config_path = base_model_path


        logger.info(f"ğŸ”„ ä½¿ç”¨ tokenizer è¨­å®šæª”: {config_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(config_path)
        self.tokenizer.padding_side = "right"

        # ç¢ºå®šæ•¸æ“šé›†åç¨±å’Œä»»å‹™
        self.dataset_name = self.get_dataset_name(self.data_file)
        self.task = self.get_task_from_dataset(self.dataset_name)
        self.instruction = self.get_instruction(self.task)
        logger.info(f"Dataset: {self.dataset_name}, Task: {self.task}")

    def load_json(self, filepath):
        """
        åŠ è¼‰ JSON æ–‡ä»¶ã€‚
        """
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"è§£æ JSON æ–‡ä»¶å¤±æ•—: {filepath}, éŒ¯èª¤: {e}")
            raise

    def load_data(self, filepath):
        """
        åŠ è¼‰æ•¸æ“šæ–‡ä»¶ã€‚
        """
        try:
            data_df = pd.read_json(filepath)
            return data_df
        except ValueError as e:
            logger.error(f"è®€å–æ•¸æ“šæ–‡ä»¶æ™‚å‡ºéŒ¯: {filepath}, éŒ¯èª¤: {e}")
            raise
        except FileNotFoundError:
            logger.error(f"æ•¸æ“šæ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            raise

    def load_labels(self, filepath):
        """
        åŠ è¼‰æ¨™ç±¤æ–‡ä»¶ä¸¦è½‰æ›ç‚º Snake_Caseã€‚
        """
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                labels = json.load(f)
            # å°‡æ¨™ç±¤è½‰æ›ç‚º Snake_Case
            labels = [label.replace(" ", "_") for label in labels]
            if len(labels) != len(set(labels)):
                logger.error("è½‰æ›å¾Œçš„æ¨™ç±¤å­˜åœ¨é‡è¤‡ï¼Œè«‹æª¢æŸ¥æ¨™ç±¤æ–‡ä»¶ã€‚")
                raise ValueError("è½‰æ›å¾Œçš„æ¨™ç±¤å­˜åœ¨é‡è¤‡ï¼Œè«‹æª¢æŸ¥æ¨™ç±¤æ–‡ä»¶ã€‚")
            return labels
        except FileNotFoundError:
            logger.error(f"æ¨™ç±¤æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"è§£ææ¨™ç±¤ JSON æ–‡ä»¶å¤±æ•—: {filepath}, éŒ¯èª¤: {e}")
            raise

    def get_dataset_name(self, data_file):
        """
        å¾ data_file è·¯å¾‘ä¸­æå–æ•¸æ“šé›†åç¨±ã€‚
        å‡è¨­è·¯å¾‘æ ¼å¼åŒ…å« dataset nameï¼Œä¾‹å¦‚ ./CL_Benchmark/TC/dbpedia/train.json
        """
        parts = data_file.split(os.sep)
        for part in parts:
            if part in self.get_all_datasets():
                return part
        logger.error(f"ç„¡æ³•å¾æ•¸æ“šæ–‡ä»¶è·¯å¾‘ä¸­æå–æ•¸æ“šé›†åç¨±: {data_file}")
        raise ValueError(f"ç„¡æ³•å¾æ•¸æ“šæ–‡ä»¶è·¯å¾‘ä¸­æå–æ•¸æ“šé›†åç¨±: {data_file}")

    def get_all_datasets(self):
        """
        å¾ task_config ä¸­ç²å–æ‰€æœ‰æ•¸æ“šé›†åç¨±ã€‚
        """
        datasets = []
        for task, dataset_list in self.task_config.items():
            for dataset in dataset_list:
                datasets.append(dataset["dataset name"])
        return datasets

    def get_task_from_dataset(self, dataset_name):
        """
        æ ¹æ“šæ•¸æ“šé›†åç¨±å¾ task_config ä¸­ç²å–å°æ‡‰çš„ä»»å‹™ã€‚
        """
        for task, dataset_list in self.task_config.items():
            for dataset in dataset_list:
                if dataset["dataset name"] == dataset_name:
                    return task
        logger.error(f"æœªæ‰¾åˆ°æ•¸æ“šé›†åç¨± '{dataset_name}' å°æ‡‰çš„ä»»å‹™ã€‚")
        raise ValueError(f"æœªæ‰¾åˆ°æ•¸æ“šé›†åç¨± '{dataset_name}' å°æ‡‰çš„ä»»å‹™ã€‚")

    def get_instruction(self, task):
        """
        æ ¹æ“šä»»å‹™å¾ instruction_config ä¸­ç²å–å°æ‡‰çš„æŒ‡ä»¤ã€‚
        """
        if task not in self.instruction_config:
            logger.error(f"æœªæ‰¾åˆ°ä»»å‹™ '{task}' çš„æŒ‡ä»¤ã€‚")
            raise ValueError(f"æœªæ‰¾åˆ°ä»»å‹™ '{task}' çš„æŒ‡ä»¤ã€‚")
        instructions = self.instruction_config[task]
        if not instructions:
            logger.error(f"ä»»å‹™ '{task}' çš„æŒ‡ä»¤åˆ—è¡¨ç‚ºç©ºã€‚")
            raise ValueError(f"ä»»å‹™ '{task}' çš„æŒ‡ä»¤åˆ—è¡¨ç‚ºç©ºã€‚")
        # å‡è¨­æ¯å€‹ä»»å‹™åªæœ‰ä¸€å€‹æŒ‡ä»¤ï¼Œé¸æ“‡ç¬¬ä¸€å€‹
        return instructions[0]["instruction"]

    def convert_label_to_id(self, example):
        """
        å°‡æ–‡å­—æ¨™ç±¤è½‰æ›ç‚ºæ•¸å­— IDã€‚
        """
        if example["label"] in self.label_to_id:
            example["label_id"] = self.label_to_id[example["label"]]
        else:
            raise ValueError(f"æœªçŸ¥æ¨™ç±¤: {example['label']}ï¼Œè«‹æª¢æŸ¥æ¨™ç±¤æ–‡ä»¶æ˜¯å¦åŒ…å«æ­¤æ¨™ç±¤ã€‚")
        return example

    def preprocess_data(self, example):
        """
        å°‡è³‡æ–™è½‰æ›ç‚ºæç¤ºå¼è¼¸å…¥ã€‚
        """
        try:
            options = ", ".join(self.labels_list)
            input_text = (
                f"Task:{self.task}\nDataset:{self.dataset_name}\n"
                f"{self.instruction}"
                f"Option: {options}\n"
                f"{example['sentence']}\nAnswer:"
            )
            # æŠŠç”¢ç”Ÿçš„ prompt ç›´æ¥åŠ é€² exampleï¼Œæ–¹ä¾¿å¾ŒçºŒ tokenize
            example["input_text"] = input_text
        except KeyError as e:
            logger.error(f"ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {e}")
            raise KeyError(f"ç¼ºå°‘å¿…è¦çš„æ¬„ä½: {e}")
        return example

    def tokenize_data(self, examples):
        """
        Hugging Face Dataset åœ¨ batched=True æ™‚ï¼Œexamples æœƒæ˜¯ä¸€æ‰¹è³‡æ–™çš„ dict of listã€‚
        æˆ‘å€‘è¦é‡å°æ•´æ‰¹ä¸€èµ· tokenizeï¼Œå†æ‹†åˆ†å›å»ã€‚
        """
        # 1. Tokenize input_text
        model_inputs = self.tokenizer(
            examples["input_text"],
            max_length=self.max_input_length,
            padding="max_length",
            truncation=True
        )

        # 2. Tokenize labels (æŠŠ label_id è½‰ç‚ºçœŸæ­£çš„æ–‡å­—æ¨™ç±¤å¾Œåš tokenize)
        label_texts = [self.labels_list[idx] for idx in examples["label_id"]]
        with self.tokenizer.as_target_tokenizer():
            labels = self.tokenizer(
                label_texts,
                max_length=self.max_label_length,
                padding="max_length",
                truncation=True
            )

        # 3. å»ºç«‹ model_inputs["labels"]ï¼Œä¸¦æŠŠ -100 mask çµ¦ padding
        label_masks = labels["attention_mask"]
        label_ids = labels["input_ids"]
        # æŠŠ padding token (= tokenizer.pad_token_id) çš„ä½ç½®æ›æˆ -100ï¼Œé¿å…å½±éŸ¿ loss
        for i in range(len(label_ids)):
            for j in range(len(label_ids[i])):
                if label_masks[i][j] == 0:
                    label_ids[i][j] = -100

        # åŠ åˆ° model_inputs
        model_inputs["labels"] = label_ids
        return model_inputs

    def get_dataset(self):
        """
        å–å¾— Datasetï¼ˆå«å‰è™•ç†èˆ‡æ¨™è¨˜åŒ–ï¼‰ã€‚
        """
        # æª¢æŸ¥æ¨™ç±¤æœ‰æ•ˆæ€§
        invalid_labels = [label for label in self.data_df["label"] if label not in self.label_to_id]
        if invalid_labels:
            # print(f"[WARNING] è³‡æ–™åŒ…å«ç„¡æ•ˆæ¨™ç±¤: {invalid_labels}")
            self.data_df = self.data_df[~self.data_df["label"].isin(invalid_labels)]

        # æª¢æŸ¥ç©ºå…§å®¹
        invalid_sentences = self.data_df["sentence"].apply(lambda x: not x.strip())
        if invalid_sentences.any():
            print(f"[WARNING] ç©ºå¥å­è¢«ç§»é™¤: {self.data_df[invalid_sentences].index.tolist()}")
            self.data_df = self.data_df[~invalid_sentences]

        # è½‰æ›æ¨™ç±¤ç‚º ID
        self.data_df = self.data_df.apply(self.convert_label_to_id, axis=1)

        # è½‰æ›æˆ Hugging Face Dataset
        hf_dataset = Dataset.from_pandas(self.data_df)

        # å…ˆé€²è¡Œ preprocess_data (å–®ç­†å³å¯)
        hf_dataset = hf_dataset.map(self.preprocess_data)

        # å†é€²è¡Œ tokenize_data (å»ºè­° batched=True)
        hf_dataset = hf_dataset.map(
            self.tokenize_data,
            batched=True,
            remove_columns=["sentence", "label", "label_id", "input_text"]  
            # é¿å…æŠŠåŸæœ¬çš„ columns å¸¶é€²å¾Œé¢çš„ DataLoader
        )
        return hf_dataset

    def get_dataloader(self, dataset, batch_size, collate_fn):
        """
        æ ¹æ“š Dataset å»ºç«‹ DataLoaderã€‚
        """
        return DataLoader(
            dataset, 
            batch_size=batch_size, 
            collate_fn=collate_fn
        )
    # ä¹‹å¾Œåˆªé™¤ï¼Œæ¸¬è©¦ç”¨
    def get_subset_dataloader(self, dataset, batch_size, collate_fn, subset_size=500, shuffle=True):
        """
        å¾ dataset ä¸­éš¨æ©ŸæŠ½å– subset_size ç­†è³‡æ–™ï¼Œä¸¦è¿”å›ç›¸æ‡‰çš„ DataLoaderã€‚
        """
        dataset_size = len(dataset)
        if subset_size > dataset_size:
            raise ValueError(f"subset_size ({subset_size}) å¤§æ–¼è³‡æ–™é›†å¤§å° ({dataset_size})")
        indices = list(range(subset_size))
        subset = Subset(dataset, indices)
        return self.get_dataloader(subset, batch_size, collate_fn)