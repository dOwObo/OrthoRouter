# main.py
import torch
from torch.nn import CrossEntropyLoss

from model.custom_t5 import CustomT5Model
from dataset.data_processor import DataProcessor
# ä½¿ç”¨ helper/utils.py çš„ collate_fn
from helper.utils import collate_fn
from helper.trainer import Trainer
import argparse
import logging
import os
import json
import shutil
from transformers import set_seed
# DEL
from model.layers import LoRALayer, Router, MoEBlock
from helper.utils import count_activated_params_t5moe

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="Train a custom T5 model with LoRA and MoE for Continual Learning.")
    parser.add_argument('--data_file', type=str, required=True, help='Path to the training data file (JSON).')
    parser.add_argument('--labels_file', type=str, required=True, help='Path to the labels file (JSON).')
    parser.add_argument('--model_path', type=str, default=None, help='Path to the pre-trained model to load (optional).')
    parser.add_argument('--output_dir', type=str, required=True, help='Directory to save the fine-tuned model.')
    parser.add_argument('--eval_file', type=str, required=True, help='Path to the evaluation data file (JSON).')
    parser.add_argument('--eval_labels_files', type=str, required=True, help='Path to the labels file (JSON).')
    parser.add_argument('--test_data_files', type=str, nargs='*', default=[], help='List of test data files (JSON).')
    parser.add_argument('--test_labels_files', type=str, nargs='*', default=[], help='List of test labels files (JSON).')
    parser.add_argument('--seed', type=int, help='seed')
    return parser.parse_args()
def save_custom_model(custom_model, output_dir):
    """
    è‡ªè¨‚ä¿å­˜æµç¨‹ï¼šä¿å­˜ state_dict å’Œè‡ªå®šç¾©é…ç½®ã€‚
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹çš„ state_dict
    state_dict_path = os.path.join(output_dir, "model_state_dict.pt")
    torch.save(custom_model.model.state_dict(), state_dict_path)
    logger.info(f"Model state_dict saved to {state_dict_path}")
    
    # ä¿å­˜è‡ªå®šç¾©é…ç½®
    config = {
        "num_experts": custom_model.num_experts,
        "expert_rank": custom_model.expert_rank  # âœ… åªä¿ç•™ expert_rank
    }
    config_path = os.path.join(output_dir, "custom_config.json")
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=4)
    logger.info(f"Custom config saved to {config_path}")


def load_custom_model(load_directory, device):
    """
    è‡ªè¨‚åŠ è¼‰æµç¨‹ï¼šåŠ è¼‰ state_dict å’Œè‡ªå®šç¾©é…ç½®ï¼Œä¸¦åˆå§‹åŒ–æ¨¡å‹ã€‚
    """
    custom_model = CustomT5Model.load_pretrained(load_directory, device=device)
    logger.info(f"Model loaded from {load_directory}")
    return custom_model

if __name__ == "__main__":
    args = parse_args()
    
    # è¨­å®šéš¨æ©Ÿç¨®å­
    set_seed(args.seed)
    
    data_file = args.data_file
    labels_file = args.labels_file
    model_path = args.model_path
    output_dir = args.output_dir
    eval_file = args.eval_file
    eval_labels_files = args.eval_labels_files
    test_data_files = args.test_data_files
    test_labels_files = args.test_labels_files

    # ç›´æ¥åˆªé™¤èˆŠçš„ `output_dir` ä¸¦é‡æ–°å»ºç«‹
    if os.path.exists(output_dir):
        print(f"ğŸ—‘ï¸ åˆªé™¤èˆŠçš„è¼¸å‡ºç›®éŒ„: {output_dir}")
        shutil.rmtree(output_dir)  # **åˆªé™¤æ•´å€‹ç›®éŒ„**
    os.makedirs(output_dir, exist_ok=True)  # **é‡æ–°å»ºç«‹æ–°çš„ç©ºç›®éŒ„**
    print(f"âœ… å·²é‡æ–°å»ºç«‹è¼¸å‡ºç›®éŒ„: {output_dir}")

    base_model_path = "./initial_model/t5-large"
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    batch_size = 8
    max_input_length = 256
    max_label_length = 50

    # è®€å–è¨“ç·´è³‡æ–™
    train_processor = DataProcessor(
        data_file=data_file,
        labels_file=labels_file,
        peft_model_path=base_model_path,
        max_input_length=max_input_length,
        max_label_length=max_label_length
    )
    train_dataset = train_processor.get_dataset()
    train_dataloader = train_processor.get_dataloader(
        train_dataset, 
        batch_size=batch_size, 
        collate_fn=collate_fn  # utils.py çš„ collate_fn
    )

    #DEL å‰µå»ºè¨“ç·´é›†çš„ 1000 ç­†å­é›†
    train_subset_dataloader = train_processor.get_subset_dataloader(
        train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        subset_size=1000,
        shuffle=True
    )

    # é©—è­‰è³‡æ–™
    eval_processor = DataProcessor(
        data_file=eval_file,
        labels_file=eval_labels_files,
        peft_model_path=base_model_path,
        max_input_length=max_input_length,
        max_label_length=max_label_length
    )
    eval_dataset = eval_processor.get_dataset()
    eval_dataloader = eval_processor.get_dataloader(
        eval_dataset, 
        batch_size=batch_size, 
        collate_fn=collate_fn,
    )

    #DEL å‰µå»ºé©—è­‰é›†çš„ 1000 ç­†å­é›†
    eval_subset_dataloader = eval_processor.get_subset_dataloader(
        eval_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        subset_size=1000,
        shuffle=True
    )

    # å»ºç«‹ T5 + LoRA æ¨¡å‹
    # å¦‚æœæœ‰æŒ‡å®šæ¨¡å‹è·¯å¾‘ï¼Œå‰‡åŠ è¼‰æ¨¡å‹ï¼Œå¦å‰‡åˆå§‹åŒ–æ–°æ¨¡å‹
    if model_path:
        logger.info(f"Loading model from {model_path}...")
        if not os.path.exists(model_path):
            logger.error(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
            raise FileNotFoundError(f"æŒ‡å®šçš„æ¨¡å‹è·¯å¾‘ä¸å­˜åœ¨: {model_path}")
        custom_model = load_custom_model(model_path, device=device)
        logger.info("Model loaded successfully.")
    else:
        logger.info("Initializing new model...")
        custom_model = CustomT5Model(base_model_path, device=device, num_experts=4, expert_rank=8)
        logger.info("Model initialized successfully.")

    model = custom_model.model  # é€™é‚Šä½¿ç”¨çš„æ˜¯ T5ForConditionalGeneration

    # è¨ˆç®— top-k å°ˆå®¶ä¸¦è¨­å®šçµ¦æ‰€æœ‰ MoEBlock
    from model.layers import Router  # ç¢ºä¿æœ‰æ­£ç¢ºåŒ¯å…¥
    hidden_dim = custom_model.model.config.d_model
    
    # ç²å–ä»»å‹™ID
    task_id = train_processor.get_task_from_dataset(train_processor.dataset_name)
    task_id_mapping = {"SC": 0, "TC": 1, "NLI": 2, "QQP": 3, "WiC": 4, "MultiRC": 5, "COPA": 6, "BoolQA": 7}
    task_id = task_id_mapping.get(task_id, 0)
    
    print(f"[Main] ç•¶å‰ä»»å‹™: {train_processor.task}, ä»»å‹™ID: {task_id}")
    
    # åˆå§‹åŒ– Router ä¸¦ç‚ºæ–°ä»»å‹™åˆå§‹åŒ–å°ˆå®¶
    router = Router(hidden_dim, num_experts=4, top_k=2)
    # ç¢ºä¿ Router åœ¨æ­£ç¢ºçš„è£ç½®ä¸Š
    router = router.to(device)
    
    # ç‚ºæ–°ä»»å‹™åˆå§‹åŒ–å°ˆå®¶å‘é‡
    router.initialize_expert_for_task(task_id, train_dataset, custom_model.model)
    
    # ä½¿ç”¨æ”¹é€²çš„ task_weight æ–¹æ³•è¨ˆç®— top-k å°ˆå®¶
    topk_experts = router.task_weight(
        dataset=train_dataset, 
        encoder_model=custom_model.model, 
        task_id=task_id, 
        strategy='confident'  # ä½¿ç”¨ç½®ä¿¡åº¦ç¯©é¸ç­–ç•¥
    )
    
    # è¨­å®šæ‰€æœ‰ MoEBlock çš„ top-k å°ˆå®¶å’Œä»»å‹™ID
    for name, module in model.named_modules():
        if isinstance(module, MoEBlock):
            module.initialize_task_experts(task_id, train_dataset, custom_model.model)
            module.set_task_top_k(topk_experts, task_id)
    
    logger.info(f"âœ… å·²ç‚ºä»»å‹™ {task_id} è¨­å®šæ‰€æœ‰ MoEBlock çš„ top-k å°ˆå®¶ã€‚")

    model = custom_model.model  

    model.config.use_cache = False
    print("use_cache:", model.config.use_cache)
    
    # å‡çµé™¤äº† LoRA/MoE ä»¥å¤–çš„åƒæ•¸
    # å‡çµæ‰€æœ‰æ¨¡å‹åƒæ•¸ then è§£å‡LoRAç›¸é—œåƒæ•¸å’Œå°ˆå®¶å±¤çš„åƒæ•¸
    logger.info("å‡çµé™¤äº† LoRA/MoE ä»¥å¤–çš„åƒæ•¸...")
    for param in model.parameters():
        param.requires_grad = False

    for name, module in model.named_modules():
        if isinstance(module, LoRALayer):
            module.lora_As.requires_grad = True
            module.lora_Bs.requires_grad = True
        elif isinstance(module, MoEBlock):
            for expert in module.experts:
                for param in expert.parameters():
                    param.requires_grad = True  # MoE å°ˆå®¶å±¤å¯è¨“ç·´

    logger.info("Parameter freezing completed.")

    # åˆå§‹åŒ– Trainer
    trainer = Trainer(
        model=model,
        # train_dataloader=train_dataloader,
        # DEL
        train_dataloader=train_subset_dataloader,
        eval_dataloader=eval_subset_dataloader,
        # eval_dataloader=eval_dataloader,
        tokenizer=train_processor.tokenizer,
        labels_list=train_processor.labels_list,
        device=device,
        task_id=task_id  # å‚³å…¥ä»»å‹™ID
    )

    # é–‹å§‹è¨“ç·´
    trainer.train(
        num_epochs=3,
        learning_rate=1e-03,         
        output_dir=output_dir,
        accumulation_steps=64
    )

    # ä¿å­˜æ¨¡å‹
    logger.info(f"Saving model to {output_dir}...")
    # custom_model.save_pretrained(output_dir)
    custom_model.model = trainer.model
    save_custom_model(custom_model, output_dir)
    # logger.info("Model saved successfully.")
    logger.info(f"Saving model with {sum(p.numel() for p in custom_model.model.parameters() if p.requires_grad)} trainable parameters")


    # æ¸¬è©¦è³‡æ–™
    if test_data_files and test_labels_files:
        if len(test_data_files) != len(test_labels_files):
            logger.error("æ¸¬è©¦æ•¸æ“šæ–‡ä»¶å’Œæ¨™ç±¤æ–‡ä»¶çš„æ•¸é‡ä¸åŒ¹é…ã€‚")
            raise ValueError("æ¸¬è©¦æ•¸æ“šæ–‡ä»¶å’Œæ¨™ç±¤æ–‡ä»¶çš„æ•¸é‡ä¸åŒ¹é…ã€‚")
        
        logger.info("Starting testing on provided datasets...")
        for test_data, test_labels in zip(test_data_files, test_labels_files):
            logger.info(f"Testing on dataset: {test_data}")
            test_processor = DataProcessor(
                data_file=test_data,
                labels_file=test_labels,
                peft_model_path=base_model_path, 
                max_input_length=max_input_length,
                max_label_length=max_label_length
            )
            test_dataset = test_processor.get_dataset()
            test_dataloader = test_processor.get_dataloader(
                test_dataset, 
                batch_size=batch_size, 
                collate_fn=collate_fn,
            )
            # DEL å‰µå»ºæ¸¬è©¦é›†çš„ 1000 ç­†å­é›†
            test_subset_dataloader = test_processor.get_subset_dataloader(
                test_dataset,
                batch_size=batch_size,
                collate_fn=collate_fn,
                subset_size=1000,
                shuffle=True
            )
            # trainer.eval_dataloader = test_dataloader
            trainer.eval_dataloader = test_subset_dataloader
            test_accuracy = trainer.validate()
            logger.info(f"Test Accuracy on {test_data}: {test_accuracy:.4f}")
    else:
        logger.info("No test datasets provided. Skipping testing.")

    # activated_params_t5moe = count_activated_params_t5moe(model)
    # print("T5MoE æ¨¡å‹æ¿€æ´»åƒæ•¸æ•¸é‡:", activated_params_t5moe)